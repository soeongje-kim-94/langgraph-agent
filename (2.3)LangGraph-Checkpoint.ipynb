{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9549c795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9187d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85abc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(model='text-embedding-3-large'),\n",
    "    collection_name='real_estate_tax_collections',\n",
    "    persist_directory='./real_estate_tax_collections'\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8026e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name='real_estate_tax_retriever',\n",
    "    description='Contains information about real estate tax up to December 2024'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a5494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/inflearn-langgraph/lib/python3.12/site-packages/langchain_tavily/tavily_research.py:97: UserWarning: Field name \"output_schema\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/inflearn-langgraph/lib/python3.12/site-packages/langchain_tavily/tavily_research.py:97: UserWarning: Field name \"stream\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    # include_answer=False,\n",
    "    # include_raw_content=False,\n",
    "    # include_images=False,\n",
    "    # include_image_descriptions=False,\n",
    "    # search_depth=\"basic\",\n",
    "    # time_range=\"day\",\n",
    "    # include_domains=None,\n",
    "    # exclude_domains=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab2bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(top_k_results=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c70e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=73320295733-1h8in52ahpt1pcqoclsiuim4em0i207m.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A56382%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.send&state=e6mbBB2hG4shpf8TO5hNkTw8k0yxux&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_community import GmailToolkit\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Can review scopes here https://developers.google.com/gmail/api/auth/scopes\n",
    "# For instance, readonly scope is 'https://www.googleapis.com/auth/gmail.readonly'\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/gmail.compose\",\n",
    "    \"https://www.googleapis.com/auth/gmail.send\"\n",
    "]\n",
    "\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    \"./google/gmail_credentials.json\",\n",
    "    SCOPES,\n",
    ")\n",
    "credentials = flow.run_local_server(port=0)\n",
    "api_resource = build(\"gmail\", \"v1\", credentials=credentials)\n",
    "gmail_toolkit = GmailToolkit(api_resource=api_resource)\n",
    "gmail_tool_list = gmail_toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_list = [retriever_tool, tavily_search_tool, arxiv_tool]\n",
    "tool_list += gmail_tool_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95656a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tool_list)\n",
    "llm_with_tools = llm.bind_tools(tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1bdb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    summary: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29737144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    아래의 대화 이력을 요약해주세요.\n",
    "    만일 기존 요약 내용이 있다면, 해당 요약을 포함해 요약해주세요.\n",
    "    \n",
    "    [대화 이력]\n",
    "    {messages}\n",
    "    \n",
    "    [기존 요약]\n",
    "    {summary}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def summarize(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    'summarize' Node\n",
    "    : 주어진 상태의 메시지를 요약한다.\n",
    "\n",
    "    Args:\n",
    "        - state(AgentState): 메시지 상태와 요약을 포함하는 state\n",
    "\n",
    "    Returns:\n",
    "        - AgentState: 응답 메시지를 포함하는 새로운 state\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state['messages']\n",
    "    summary = state['summary']\n",
    "    \n",
    "    summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
    "    ai_message = summarize_chain.invoke({'messages': messages, 'summary': summary})\n",
    "    \n",
    "    return {'summary': ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae0d9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def delete(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    'delete' Node\n",
    "    : 주어진 상태에서 오래된 메시지를 삭제한다.\n",
    "\n",
    "    Args:\n",
    "        - state(AgentState): 메시지 상태와 요약을 포함하는 state\n",
    "\n",
    "    Returns:\n",
    "        - AgentState: 오래된 메시지가 삭제된 새로운 state\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state['messages']\n",
    "    \n",
    "    return {'messages': [RemoveMessage(id=message.id) for message in messages[:-3]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e13e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    'agent' Node\n",
    "    : 주어진 상태에서 메시지를 가져와 LLM 및 도구를 사용하여 응답 메시지를 생성한다.\n",
    "\n",
    "    Args:\n",
    "        - state(AgentState): 메시지 상태와 요약을 포함하는 state\n",
    "\n",
    "    Returns:\n",
    "        - AgentState: 응답 메시지를 포함하는 새로운 state\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state['messages']\n",
    "    summary = state['summary']\n",
    "    \n",
    "    if summary != '':\n",
    "        messages = [SystemMessage(content=f'Here is the summary of the earlier conversation: {summary}')] + messages\n",
    "    \n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {'messages': [ai_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6663e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal['tools', 'summarize']:\n",
    "    \"\"\"\n",
    "    주어진 state에 따라 다음 단계로 진행할지 결정한다.\n",
    "\n",
    "    Args:\n",
    "        - state(AgentState): 메시지와 도구 호출 정보를 포함하는 state\n",
    "\n",
    "    Returns:\n",
    "        - Literal['tools', 'summarize']: 다음 단계로 'tools' 또는 'summarize'를 반환\n",
    "    \"\"\"\n",
    "\n",
    "    messages = state['messages']\n",
    "    last_ai_message = messages[-1]\n",
    "\n",
    "    return 'tools' if last_ai_message.tool_calls else 'summarize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8030a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x113ed76e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# nodes\n",
    "graph_builder.add_node('agent', agent)\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "graph_builder.add_node(summarize)\n",
    "graph_builder.add_node(delete)\n",
    "\n",
    "# edges\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'agent',\n",
    "    should_continue,\n",
    "    ['tools', 'summarize'] \n",
    ")\n",
    "graph_builder.add_edge('tools', 'agent')\n",
    "graph_builder.add_edge('summarize', 'delete')\n",
    "graph_builder.add_edge('delete', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8a30f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f11a4c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x114228e90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32fd825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "'Attention Is All You Need' 논문을 요약해서 메일 초안을 작성해주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_JoyNbDqPWKXPNGzOo49vV8aa)\n",
      " Call ID: call_JoyNbDqPWKXPNGzOo49vV8aa\n",
      "  Args:\n",
      "    query: Attention Is All You Need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2021-05-06\n",
      "Title: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\n",
      "Authors: Luke Melas-Kyriazi\n",
      "Summary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\% top-1 accuracy, compared to 77.9\\% and 79.9\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\n",
      "\n",
      "Published: 2025-12-03\n",
      "Title: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\n",
      "Authors: Anton Alyakin\n",
      "Summary: The 2017 paper ''Attention Is All You Need'' introduced the Transformer architecture-and inadvertently spawned one of machine learning's most persistent naming conventions. We analyze 717 arXiv preprints containing ''All You Need'' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical ''X [Is] All You Need'' structure, ''Attention'' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern's success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\n",
      "\n",
      "Published: 2018-06-28\n",
      "Title: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\n",
      "Authors: Serena Wang, Maya Gupta, Seungil You\n",
      "Summary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC's joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees' ordering.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "'Attention Is All You Need' 논문에 대한 요약을 포함한 메일 초안을 아래와 같이 작성했습니다.\n",
      "\n",
      "---\n",
      "\n",
      "**메일 초안**\n",
      "\n",
      "제목: 'Attention Is All You Need' 논문 요약\n",
      "\n",
      "안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다.\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "---\n",
      "\n",
      "이 메일 초안은 필요에 따라 수정하거나 추가할 수 있습니다.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "'Attention Is All You Need' 논문에 대한 요약을 포함한 메일 초안을 아래와 같이 작성했습니다.\n",
      "\n",
      "---\n",
      "\n",
      "**메일 초안**\n",
      "\n",
      "제목: 'Attention Is All You Need' 논문 요약\n",
      "\n",
      "안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다.\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "---\n",
      "\n",
      "이 메일 초안은 필요에 따라 수정하거나 추가할 수 있습니다.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "'Attention Is All You Need' 논문에 대한 요약을 포함한 메일 초안을 아래와 같이 작성했습니다.\n",
      "\n",
      "---\n",
      "\n",
      "**메일 초안**\n",
      "\n",
      "제목: 'Attention Is All You Need' 논문 요약\n",
      "\n",
      "안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다.\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "---\n",
      "\n",
      "이 메일 초안은 필요에 따라 수정하거나 추가할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"'Attention Is All You Need' 논문을 요약해서 메일 초안을 작성해주세요.\"\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'summarize_paper'\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream({'messages': [HumanMessage(query)], 'summary': ''}, config=config, stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d4cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1878, 'total_tokens': 1896, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1792}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Cufe5O20d42fq9FAlHj8aaTVcBbrG', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b8e82-b594-7da0-b502-3bfe836a2db8-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Attention Is All You Need'}, 'id': 'call_JoyNbDqPWKXPNGzOo49vV8aa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1878, 'output_tokens': 18, 'total_tokens': 1896, 'input_token_details': {'audio': 0, 'cache_read': 1792}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Published: 2021-05-06\\nTitle: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\\nAuthors: Luke Melas-Kyriazi\\nSummary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\\\% top-1 accuracy, compared to 77.9\\\\% and 79.9\\\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\\n\\nPublished: 2025-12-03\\nTitle: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\\nAuthors: Anton Alyakin\\nSummary: The 2017 paper \\'\\'Attention Is All You Need\\'\\' introduced the Transformer architecture-and inadvertently spawned one of machine learning\\'s most persistent naming conventions. We analyze 717 arXiv preprints containing \\'\\'All You Need\\'\\' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical \\'\\'X [Is] All You Need\\'\\' structure, \\'\\'Attention\\'\\' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern\\'s success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\\n\\nPublished: 2018-06-28\\nTitle: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\\nAuthors: Serena Wang, Maya Gupta, Seungil You\\nSummary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC\\'s joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees\\' ordering.', name='arxiv', id='f0112948-558d-42f5-912b-41dbdbafeebf', tool_call_id='call_JoyNbDqPWKXPNGzOo49vV8aa'),\n",
       " AIMessage(content=\"'Attention Is All You Need' 논문에 대한 요약을 포함한 메일 초안을 아래와 같이 작성했습니다.\\n\\n---\\n\\n**메일 초안**\\n\\n제목: 'Attention Is All You Need' 논문 요약\\n\\n안녕하세요,\\n\\n최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다.\\n\\n이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\\n\\n또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\\n\\n결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\\n\\n이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\\n\\n감사합니다.\\n\\n---\\n\\n이 메일 초안은 필요에 따라 수정하거나 추가할 수 있습니다.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 2615, 'total_tokens': 2959, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1792}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Cufe6Z66ZbQAGUt9FxWhIdIDZtRjc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8e82-b943-72c2-afcd-a940cf50a926-0', usage_metadata={'input_tokens': 2615, 'output_tokens': 344, 'total_tokens': 2959, 'input_token_details': {'audio': 0, 'cache_read': 1792}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38e58835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대화 이력에서는 \"Attention Is All You Need\" 논문에 대한 요약을 요청한 후, AI가 해당 논문의 내용을 바탕으로 메일 초안을 작성한 과정이 담겨 있습니다. 작성된 메일 초안에서는 Transformer 아키텍처의 특징과 그 혁신적인 접근 방식, 자기 주의 메커니즘 및 다중 머리 주의가 NLP 분야에서의 성능 향상에 기여한 점이 강조되었습니다. 또한, 해당 논문이 딥러닝과 NLP 연구에 미친 영향에 대해서도 언급되었습니다. AI가 생성한 메일 초안은 수신자가 쉽게 이해할 수 있도록 내용을 정리했으며, 필요에 따라 수정 가능한 형태로 제공되었습니다.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc18025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "논문의 출처 URL을 포함시켜주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "아래는 URL을 추가한 메일 초안입니다.\n",
      "\n",
      "---\n",
      "\n",
      "**메일 초안**\n",
      "\n",
      "제목: 'Attention Is All You Need' 논문 요약\n",
      "\n",
      "안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다. 해당 논문은 다음 링크에서 확인하실 수 있습니다: [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "--- \n",
      "\n",
      "이제 메일 초안에 논문의 출처 URL이 포함되었습니다. 필요에 따라 수정하시면 됩니다.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "아래는 URL을 추가한 메일 초안입니다.\n",
      "\n",
      "---\n",
      "\n",
      "**메일 초안**\n",
      "\n",
      "제목: 'Attention Is All You Need' 논문 요약\n",
      "\n",
      "안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다. 해당 논문은 다음 링크에서 확인하실 수 있습니다: [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "--- \n",
      "\n",
      "이제 메일 초안에 논문의 출처 URL이 포함되었습니다. 필요에 따라 수정하시면 됩니다.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "아래는 URL을 추가한 메일 초안입니다.\n",
      "\n",
      "---\n",
      "\n",
      "**메일 초안**\n",
      "\n",
      "제목: 'Attention Is All You Need' 논문 요약\n",
      "\n",
      "안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다. 해당 논문은 다음 링크에서 확인하실 수 있습니다: [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "--- \n",
      "\n",
      "이제 메일 초안에 논문의 출처 URL이 포함되었습니다. 필요에 따라 수정하시면 됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"논문의 출처 URL을 포함시켜주세요.\"\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'summarize_paper'\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream({'messages': [HumanMessage(query)]}, config=config, stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22f35beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대화 이력에서는 \"Attention Is All You Need\" 논문에 대한 요약을 요청한 후, AI가 해당 논문의 내용을 바탕으로 메일 초안을 작성한 과정이 담겨 있습니다. 작성된 메일 초안에서는 Transformer 아키텍처의 특징과 그 혁신적인 접근 방식, 자기 주의 메커니즘 및 다중 머리 주의가 NLP 분야에서의 성능 향상에 기여한 점이 강조되었습니다. 또한, 해당 논문이 딥러닝과 NLP 연구에 미친 영향에 대해서도 언급되었고, 논문의 출처 URL이 포함되어 제공되었습니다. AI가 생성한 메일 초안은 수신자가 쉽게 이해할 수 있도록 내용을 정리했으며, 필요에 따라 수정 가능한 형태로 제공되었습니다.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a5400b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "tjdwp3s@gmail.com으로 전송해주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  send_gmail_message (call_Zw6oHvoMpJg3XPjv8fsF8PyT)\n",
      " Call ID: call_Zw6oHvoMpJg3XPjv8fsF8PyT\n",
      "  Args:\n",
      "    message: 안녕하세요,\n",
      "\n",
      "최근 'Attention Is All You Need'라는 논문을 읽고 그 내용을 간단히 요약하여 공유하고자 합니다. 해당 논문은 다음 링크에서 확인하실 수 있습니다: [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
      "\n",
      "이 논문은 Transformer 아키텍처를 소개하며, 자연어 처리(NLP) 분야에서의 혁신적인 접근 방식을 제시합니다. Transformer는 순차적 처리 대신 전체 입력 시퀀스에 동시에 주의를 집중할 수 있는 자기 주의 메커니즘(self-attention mechanism)을 사용합니다. 이 방식은 병렬 처리를 가능하게 하여 모델의 학습 속도를 크게 향상시킵니다.\n",
      "\n",
      "또한, 다중 머리 주의(multi-head attention)를 통해 모델이 입력의 다양한 부분에 동시에 주의를 기울일 수 있도록 하여, 정보의 함의를 좀 더 유연하게 파악할 수 있게 합니다. 이러한 구조 덕분에 Transformer는 기계 번역, 텍스트 생성 등 여러 NLP 작업에서 뛰어난 성능을 보이며, 후속 연구와 모델의 기초가 되었습니다.\n",
      "\n",
      "결론적으로, 이 논문은 딥러닝과 NLP 연구에 큰 영향을 미쳤으며, 주의 메커니즘을 통해 새로운 가능성을 열어줍니다.\n",
      "\n",
      "이 요약이 도움이 되길 바라며, 더 궁금한 사항이 있으시면 말씀해 주세요.\n",
      "\n",
      "감사합니다.\n",
      "    to: tjdwp3s@gmail.com\n",
      "    subject: 'Attention Is All You Need' 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: send_gmail_message\n",
      "\n",
      "Message sent. Message Id: 19b8e89a327ee81c\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "메일이 성공적으로 전송되었습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "메일이 성공적으로 전송되었습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "메일이 성공적으로 전송되었습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"tjdwp3s@gmail.com으로 전송해주세요.\"\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'summarize_paper'\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream({'messages': [HumanMessage(query)]}, config=config, stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e616d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values['summary']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inflearn-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
